{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaZUMypAlcXbfnffvB1o5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajwayne/GPT-3-/blob/main/GPT3_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTMEhmPKdxmD"
      },
      "outputs": [],
      "source": [
        "pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install prompt_toolkit"
      ],
      "metadata": {
        "id": "MhsczfEwgyHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#testing "
      ],
      "metadata": {
        "id": "_KoV3_UmArtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import secrets\n",
        "ex = secrets.api_key"
      ],
      "metadata": {
        "id": "C_ePrEJ_EEeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import secrets\n",
        "# Set up OpenAI API key\n",
        "openai.api_key = secrets.api_key\n",
        "# Test the connection\n",
        "models = openai.Model.list()\n",
        "print(models['data'])"
      ],
      "metadata": {
        "id": "c_R6WA2pjVs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment "
      ],
      "metadata": {
        "id": "GXaq3VWsAui-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from prompt_toolkit import prompt\n",
        "\n",
        "# Set up OpenAI API key\n",
        "openai.api_key = secrets.api_key\n",
        "\n",
        "# Define the GPT-3 prompt\n",
        "prompt_text = \"Please ask me a question.\""
      ],
      "metadata": {
        "id": "4Ve1elxCAqAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GPT-3 parameters\n",
        "model_engine = \"text-davinci-002\"\n",
        "temperature = 0.5\n",
        "max_tokens = 50"
      ],
      "metadata": {
        "id": "T77y3QM8G0N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a tagline for an IceCream shop\""
      ],
      "metadata": {
        "id": "zIX5s_CVLSSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(engine = \"text-davinci-001\", prompt = prompt, max_tokens =6)"
      ],
      "metadata": {
        "id": "DSUh7eTqLynS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "5I373vxfL3VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic "
      ],
      "metadata": {
        "id": "bd0eg5fpCYKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below Code takes in user input and prints the output "
      ],
      "metadata": {
        "id": "9eImvTY4PliQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8mCa1PyPk58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt): #This funtion generates text given a prompt \n",
        "    response = openai.Completion.create( #creates a completion request to generate text using the \"davinci\" engine\n",
        "        engine=\"davinci\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    text = response.choices[0].text #The function is simply taking the first choice and extracting the generated text.\n",
        "    text = re.sub('[^0-9a-zA-Z\\n\\.\\?,!]+', ' ', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "t7VYBvl4AdpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \") #User Input \n",
        "    prompt = f\"You said: {user_input}\\nAI: \" \n",
        "\n",
        "    generated_text = generate_text(prompt)\n",
        "    print(generated_text)"
      ],
      "metadata": {
        "id": "xWhcqrFJAe4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimized "
      ],
      "metadata": {
        "id": "O0w8w5klCcpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "b3ghcDNyBLiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmVi6srKBLfd",
        "outputId": "f13e8ac0-ada9-490f-df77-5cf6c3716d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "9IVhNmb-BLdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input(user_input):\n",
        "    # Remove stop words\n",
        "    words = word_tokenize(user_input)\n",
        "    filtered_words = [word for word in words if not word.lower() in stop_words]\n",
        "\n",
        "    # Perform lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "    # Join the words back into a sentence\n",
        "    return ' '.join(lemmatized_words)"
      ],
      "metadata": {
        "id": "IVuQqMmEBLbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(user_input):\n",
        "    # Use Named Entity Recognition to extract named entities from user input\n",
        "    entities = []\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "TacX_aGqBLYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(user_input):\n",
        "    # Use Sentiment Analysis to analyze the emotional tone of the user input\n",
        "    sentiment = 'neutral'\n",
        "\n",
        "    return sentiment\n"
      ],
      "metadata": {
        "id": "vNDlzNZIBLVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    text = response.choices[0].text\n",
        "    text = re.sub('[^0-9a-zA-Z\\n\\.\\?,!]+', ' ', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "nqgiGQKBBLS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    \n",
        "    # Preprocess the user input to remove stop words and perform lemmatization\n",
        "    preprocessed_input = preprocess_input(user_input)\n",
        "\n",
        "    # Extract named entities from the user input\n",
        "    entities = extract_entities(user_input)\n",
        "\n",
        "    # Analyze the sentiment of the user input\n",
        "    sentiment = analyze_sentiment(user_input)\n",
        "\n",
        "    # Build the prompt using the preprocessed input, entities, and sentiment\n",
        "    prompt = f\"You said: {preprocessed_input}\\nEntities: {entities}\\nSentiment: {sentiment}\\nAI: \"\n",
        "\n",
        "    # Generate the response using GPT-3\n",
        "    generated_text = generate_text(prompt)\n",
        "\n",
        "    # Print the response\n",
        "    print(generated)"
      ],
      "metadata": {
        "id": "n4Q2mKLuBLPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5W7GGrR1BLLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tkinter as tk\n",
        "from tkinter import scrolledtext\n",
        "import openai\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_input(user_input):\n",
        "    # Remove stop words\n",
        "    words = word_tokenize(user_input)\n",
        "    filtered_words = [word for word in words if not word.lower() in stop_words]\n",
        "\n",
        "    # Perform lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "    # Join the words back into a sentence\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "def extract_entities(user_input):\n",
        "    # Use Named Entity Recognition to extract named entities from user input\n",
        "    entities = []\n",
        "\n",
        "    return entities\n",
        "\n",
        "def analyze_sentiment(user_input):\n",
        "    # Use Sentiment Analysis to analyze the emotional tone of the user input\n",
        "    sentiment = 'neutral'\n",
        "\n",
        "    return sentiment\n",
        "\n",
        "def generate_text(prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.5,\n",
        "        max_tokens=1024,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    text = response.choices[0].text\n",
        "    text = re.sub('[^0-9a-zA-Z\\n\\.\\?,!]+', ' ', text)\n",
        "    return text\n",
        "\n",
        "def generate_response():\n",
        "    # Get the user input from the text box\n",
        "    user_input = user_input_text.get('1.0', tk.END).strip()\n",
        "\n",
        "    # Preprocess the user input to remove stop words and perform lemmatization\n",
        "    preprocessed_input = preprocess_input(user_input)\n",
        "\n",
        "    # Extract named entities from the user input\n",
        "    entities = extract_entities(user_input)\n",
        "\n",
        "    # Analyze the sentiment of the user input\n",
        "    sentiment = analyze_sentiment(user_input)\n",
        "\n",
        "    # Build the prompt using the preprocessed input, entities, and sentiment\n",
        "    prompt = f\"You said: {preprocessed_input}\\nEntities: {entities}\\nSentiment: {sentiment}\\nAI: \"\n",
        "\n",
        "    # Generate the response using GPT-3\n",
        "    generated_text = generate_text(prompt)\n",
        "\n",
        "    # Add the generated response to the text box\n",
        "    ai_response_text.insert(tk.END, f\"AI: {generated_text}\\n\\n\")\n",
        "\n",
        "# Create the main window\n",
        "root = tk.Tk()\n",
        "root.title(\"AI Chatbot\")\n",
        "\n",
        "# Create the user input text box\n",
        "user_input_label = tk.Label(root, text=\"You:\")\n",
        "user_input_label.grid(row=0, column=0, sticky=tk.W)\n",
        "user_input_text = tk.Text(root, height=5)\n",
        "user_input_text.grid(row=1, column=0, padx=10, pady=10)\n",
        "\n",
        "# Create the AI response text box\n",
        "ai_response_label = tk.Label(root, text=\"AI:\")\n",
        "ai_response_label.grid(row=2, column=0, sticky=tk.W)\n",
        "ai_response_text = scrolledtext.ScrolledText(root, height=10)\n",
        "ai_response_text.grid(row=3, column=0, padx=10, pady=10)\n",
        "\n",
        "# Create the send button\n",
        "send_button = tk.Button(root, text=\"Send\", command=generate_response)\n",
        "send_button.grid(row=4, column=0, pady=10)\n"
      ],
      "metadata": {
        "id": "6kjSKJDxDK_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}